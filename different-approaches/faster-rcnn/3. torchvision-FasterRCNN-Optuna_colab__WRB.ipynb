{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Google Colab\n",
    "\n",
    "First, mount Google Drive to access files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "# project_name = \"SeamTaping\"\n",
    "project_name = \"WRB\"\n",
    "print(\"Project:\", project_name)\n",
    "\n",
    "# Path to saved images\n",
    "image_folder = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/images'\n",
    "\n",
    "# Load dataset from JSON\n",
    "train_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/train_data.json'\n",
    "val_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/val_data.json'\n",
    "test_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/test_data.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Dataset Class\n",
    "\n",
    "Create a custom dataset class to load images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset_json_path, image_folder):\n",
    "        with open(dataset_json_path, 'r') as f:\n",
    "            dataset = json.load(f)\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.image_folder = image_folder\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.image_size = (800, 800)\n",
    "        self.transforms = T.Compose([\n",
    "            T.Resize(self.image_size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=self.mean, std=self.std)\n",
    "        ])\n",
    "\n",
    "        self.label_map = {\n",
    "            'WRB-Bad': 0,\n",
    "            # Add more labels as needed\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def xywh_to_xyxy(self, xywh):\n",
    "        x, y, w, h = xywh\n",
    "        x2 = x + w\n",
    "        y2 = y + h\n",
    "        xyxy = [x, y, x2, y2]\n",
    "        return xyxy\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_data = self.dataset[idx]\n",
    "        image_file_name = image_data['image_file_name']\n",
    "        image_path = os.path.join(self.image_folder, image_file_name)\n",
    "\n",
    "        # Load image\n",
    "        image_original = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in image_data['annotations']:\n",
    "            bbox = annotation['bbox']\n",
    "            box = self.xywh_to_xyxy(bbox)\n",
    "            boxes.append(box)\n",
    "            labels.append(self.label_map[annotation['label']])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image_original)\n",
    "\n",
    "        # Calculate scaling factor for resizing bounding boxes AFTER transforms\n",
    "        original_size = np.array(image_original.size)  # Get original size from the image file\n",
    "        # print(original_size)\n",
    "        resized_size = self.image_size\n",
    "        scale = resized_size / original_size\n",
    "        # print(scale)\n",
    "\n",
    "        # Adjust bounding box coordinates based on resizing\n",
    "        boxes[:, 0] *= scale[0]  # x_min\n",
    "        boxes[:, 1] *= scale[1]  # y_min\n",
    "        boxes[:, 2] *= scale[0]  # x_max\n",
    "        boxes[:, 3] *= scale[1]  # y_max\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Create custom dataset instance with augmentation enabled\n",
    "train_dataset = CustomDataset(train_dataset_json_path, image_folder)\n",
    "val_dataset = CustomDataset(val_dataset_json_path, image_folder)\n",
    "test_dataset = CustomDataset(test_dataset_json_path, image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Objective Function\n",
    "First, define the objective function that Optuna will optimize. In this case, the objective function will train the FasterRCNN model with given hyperparameters and return the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-3)\n",
    "    steps = trial.suggest_int('steps', 500, 2000, 500)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [2, 4, 8])\n",
    "\n",
    "    \n",
    "    dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Define model and optimizer\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    num_classes = 1  # 1 class (change accordingly if more classes)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(steps):\n",
    "        for images, targets in dataloader:\n",
    "            images = list(image for image in images)\n",
    "            targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Validation (calculate validation loss)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_dataloader:  # Replace with your validation dataloader\n",
    "            images = list(image for image in images)\n",
    "            targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return val_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Azure-Custom-Vision-Practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
