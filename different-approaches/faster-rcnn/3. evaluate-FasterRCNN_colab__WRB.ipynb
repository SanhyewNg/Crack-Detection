{"cells":[{"cell_type":"markdown","metadata":{"id":"x1-Xf8_oClDx"},"source":["## Setup Google Colab\n","\n","First, mount Google Drive to access files:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6019,"status":"ok","timestamp":1720109149682,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"U2BoLZEoClD2","outputId":"a1427d7f-c212-4b53-c17a-4aed826f369e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","Project: WRB\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# project_name = \"SeamTaping\"\n","project_name = \"WRB\"\n","print(\"Project:\", project_name)\n","\n","# Path to saved images\n","image_folder = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/images'\n","\n","# Load dataset from JSON\n","train_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/train_data.json'\n","val_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/val_data.json'\n","test_dataset_json_path = f'/content/gdrive/MyDrive/CrackDetection/{project_name}_dataset/test_data.json'\n"]},{"cell_type":"markdown","metadata":{"id":"6UvHiREqClD4"},"source":["## Define Custom Dataset Class\n","\n","Create a custom dataset class to load images and annotations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFgyqDEMClD4"},"outputs":[],"source":["import os\n","import json\n","import numpy as np\n","import torch\n","from PIL import Image, ImageDraw\n","import torchvision.transforms as T\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataset_json_path, image_folder):\n","        with open(dataset_json_path, 'r') as f:\n","            dataset = json.load(f)\n","\n","        self.dataset = dataset\n","        self.image_folder = image_folder\n","        self.mean = [0.485, 0.456, 0.406]\n","        self.std = [0.229, 0.224, 0.225]\n","        self.image_size = (800, 800)\n","        self.transforms = T.Compose([\n","            T.Resize(self.image_size),\n","            T.ToTensor(),\n","            T.Normalize(mean=self.mean, std=self.std)\n","        ])\n","\n","        self.label_map = {\n","            'WRB-Bad': 1,\n","            # Add more labels as needed\n","        }\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def xywh_to_xyxy(self, xywh):\n","        x, y, w, h = xywh\n","        x2 = x + w\n","        y2 = y + h\n","        xyxy = [x, y, x2, y2]\n","        return xyxy\n","\n","    def __getitem__(self, idx):\n","        image_data = self.dataset[idx]\n","        image_file_name = image_data['image_file_name']\n","        image_path = os.path.join(self.image_folder, image_file_name)\n","\n","        # Load image\n","        image_original = Image.open(image_path).convert(\"RGB\")\n","        # Apply transformations\n","        if self.transforms is not None:\n","            image = self.transforms(image_original)\n","\n","        # Calculate scaling factor for resizing bounding boxes AFTER transforms\n","        original_size = np.array(image_original.size)  # Get original size from the image file\n","        # print(original_size)\n","        resized_size = self.image_size\n","        scale = resized_size / original_size\n","        # print(scale)\n","\n","\n","        # Get bounding boxes and labels\n","        boxes = []\n","        labels = []\n","        for annotation in image_data['annotations']:\n","            bbox = annotation['bbox']\n","            box = self.xywh_to_xyxy(bbox)\n","            # Adjust bounding box coordinates based on resizing\n","            box[0] *= scale[0]  # x_min\n","            box[1] *= scale[1]  # y_min\n","            box[2] *= scale[0]  # x_max\n","            box[3] *= scale[1]  # y_max\n","\n","            boxes.append(box)\n","            labels.append(self.label_map[annotation['label']])\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.int64)\n","        target = {\n","            'boxes': boxes,\n","            'labels': labels\n","        }\n","\n","        return image, target\n","\n","# Create custom dataset instance with augmentation enabled\n","train_dataset = CustomDataset(train_dataset_json_path, image_folder)\n","val_dataset = CustomDataset(val_dataset_json_path, image_folder)\n","test_dataset = CustomDataset(test_dataset_json_path, image_folder)\n","\n","def collate_fn(batch):\n","    images = [item[0] for item in batch]\n","    targets = [item[1] for item in batch]\n","\n","    # Assuming targets is a list of dictionaries\n","    for idx, target in enumerate(targets):\n","        # Convert target to a format suitable for the model\n","        targets[idx] = {\n","            'boxes': target['boxes'].clone().detach().to(torch.float32),  # Ensure boxes are float32\n","            'labels': target['labels'].clone().detach().to(torch.int64),  # Ensure boxes are int64\n","            # Add other keys as necessary (e.g., masks, keypoints)\n","        }\n","\n","    return images, targets\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n","val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"]},{"cell_type":"markdown","metadata":{"id":"uu24PpE2ClD6"},"source":["## Train TorchVision FasterRCNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3U2um7zClD7"},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","import torchvision\n","\n","def get_model(weights=None):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"YoosJen0ClD-"},"source":["## Load and Evaluate the Model\n"]},{"cell_type":"markdown","metadata":{"id":"_JZBXA3CClD_"},"source":["### Load the model for inference."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1761,"status":"ok","timestamp":1720109151440,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"uH9Zkq2uClD_","outputId":"b880f6dc-057d-44fb-c843-a89ceedffefe"},"outputs":[{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","  )\n",")"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# Load model\n","model = get_model()\n","\n","checkpoint_dir = '/content/gdrive/MyDrive/CrackDetection'\n","# _ = model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'faster_rcnn_model_WRB.pth'))) # if gpu is available\n","_ = model.load_state_dict(torch.load(os.path.join(checkpoint_dir, f'faster_rcnn_model_WRB.pth'), map_location=torch.device('cpu'))) # if gpu is not available\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"lmfwxW4uClD_"},"source":["### Evaluate the trained model\n","\n","using metrics like accuracy, precision, recall, and F1-score."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16019,"status":"ok","timestamp":1720109167456,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"5mPMRqVtClD_","outputId":"421f4dc2-5bac-4de1-f688-1c9a45efc185"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'boxes': tensor([[367.3355, 340.1585, 400.5655, 374.7926],\n","        [333.2627, 373.3300, 353.4489, 391.5261],\n","        [409.2523, 365.2770, 447.5489, 428.9388],\n","        [395.2663, 232.9047, 460.6442, 289.9859]]), 'labels': tensor([1, 1, 1, 1])}, {'boxes': tensor([[332.4328, 417.6826, 358.7670, 491.6311]]), 'labels': tensor([1])}]\n","[{'boxes': tensor([[409.8213, 357.2304, 449.1165, 430.6518],\n","        [392.0437, 210.3670, 460.4063, 420.0887],\n","        [364.6435, 338.2320, 401.9261, 381.4536],\n","        [561.8439, 459.6078, 574.3701, 479.9553],\n","        [328.3272, 330.3872, 445.0329, 438.3831],\n","        [331.7285, 374.9957, 350.6353, 392.8164],\n","        [451.0667, 496.8382, 464.9714, 511.9885],\n","        [327.8813, 337.8919, 407.8651, 408.5976],\n","        [ 33.1398, 388.7067,  52.1851, 414.1129],\n","        [557.9310, 456.5788, 578.0540, 486.0414],\n","        [428.4923, 360.7051, 449.7566, 422.6173],\n","        [335.2968, 214.2647, 458.3116, 437.4513],\n","        [390.0216, 213.9999, 458.1707, 290.6931],\n","        [563.0065, 461.2660, 572.1845, 475.1164],\n","        [447.4962, 492.5281, 468.6929, 515.8289],\n","        [357.1110, 419.6632, 366.7544, 442.3581],\n","        [330.4315, 370.5042, 359.1305, 401.0913],\n","        [392.8570, 230.9166, 438.9949, 378.3033],\n","        [326.4493, 361.9904, 371.5947, 408.7963],\n","        [350.3863, 412.8242, 371.0047, 446.6599],\n","        [330.7014, 350.6859, 381.1067, 429.1433],\n","        [ 24.8368, 379.8383,  58.8103, 419.5815],\n","        [702.0591, 426.5609, 712.6904, 439.1704],\n","        [576.0404, 172.8732, 583.6854, 185.5894],\n","        [453.9410, 499.9397, 462.8319, 509.8570],\n","        [341.2632, 373.6578, 371.9664, 445.9223],\n","        [557.1179, 496.7427, 567.4221, 506.5355],\n","        [353.0087, 340.5926, 407.4599, 443.7413],\n","        [545.0414, 445.3021, 584.7571, 494.2260],\n","        [397.1250, 234.2075, 439.6890, 278.1056],\n","        [394.8986, 309.8938, 450.6913, 429.0517],\n","        [350.6092, 332.6814, 433.8420, 399.0501],\n","        [359.8998, 390.2548, 370.0738, 412.1495],\n","        [385.3315, 182.6049, 449.9074, 358.1462],\n","        [360.3152, 352.4556, 451.2477, 426.2369]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.8238, 0.7759, 0.7584, 0.6884, 0.6863, 0.6819, 0.6602, 0.6282, 0.5527,\n","        0.5425, 0.4696, 0.4660, 0.4006, 0.3648, 0.3499, 0.3394, 0.3243, 0.2054,\n","        0.1909, 0.1710, 0.1396, 0.1362, 0.1221, 0.1187, 0.1144, 0.1134, 0.0981,\n","        0.0864, 0.0834, 0.0785, 0.0712, 0.0708, 0.0588, 0.0583, 0.0572])}, {'boxes': tensor([[335.0481, 419.2283, 357.2599, 489.1860],\n","        [540.2884,  84.7234, 691.2729, 192.7372],\n","        [442.6536, 526.1610, 490.1807, 600.0404],\n","        [328.5861, 405.7038, 366.2573, 500.8910],\n","        [568.9524, 117.5578, 657.2013, 193.3038],\n","        [529.2874,  85.4369, 694.6003, 297.0766],\n","        [  5.6664, 435.2430,  16.0542, 452.4464],\n","        [183.8952,  33.1731, 192.4164,  48.3349],\n","        [444.2952, 163.4711, 591.5342, 599.9596],\n","        [  2.5636, 432.3207,  19.0571, 455.3421]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': tensor([0.9508, 0.8282, 0.1731, 0.1568, 0.1070, 0.0844, 0.0833, 0.0579, 0.0541,\n","        0.0531])}]\n"]}],"source":["  model.eval()\n","\n","  with torch.no_grad():\n","      for images, targets in test_dataloader:\n","          images = list(image.to(device) for image in images)\n","          ground_truths = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          print(ground_truths)\n","\n","          predictions = model(images)\n","          print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1720109167456,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"3n38PyqGIAGX","outputId":"2186e43a-0065-4206-a5f0-b6ea2d5bb2fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'boxes': array([[367.33548, 340.15854, 400.56546, 374.7926 ],\n","       [333.26273, 373.33005, 353.44894, 391.5261 ],\n","       [409.25235, 365.277  , 447.54892, 428.9388 ],\n","       [395.26633, 232.90468, 460.64423, 289.9859 ]], dtype=float32), 'labels': array([1, 1, 1, 1])}, {'boxes': array([[332.4328 , 417.68265, 358.76697, 491.63113]], dtype=float32), 'labels': array([1])}]\n"]}],"source":["for ground_truth in ground_truths:\n","    ground_truth['boxes'] = ground_truth['boxes'].cpu().numpy()\n","    ground_truth['labels'] = ground_truth['labels'].cpu().numpy()\n","\n","print(ground_truths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1720109167456,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"urcFeI0XL7YI","outputId":"953ca0e8-4ea7-4425-cc65-7f2ee417a702"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'boxes': array([[409.82132 , 357.2304  , 449.1165  , 430.6518  ],\n","       [392.0437  , 210.36697 , 460.4063  , 420.08865 ],\n","       [364.6435  , 338.23196 , 401.92615 , 381.45364 ],\n","       [561.84393 , 459.60782 , 574.37006 , 479.9553  ],\n","       [328.3272  , 330.38718 , 445.0329  , 438.38315 ],\n","       [331.72852 , 374.9957  , 350.63525 , 392.81638 ],\n","       [451.0667  , 496.8382  , 464.97144 , 511.98846 ],\n","       [327.88135 , 337.89188 , 407.86505 , 408.59763 ],\n","       [ 33.13978 , 388.70667 ,  52.18512 , 414.1129  ],\n","       [557.93097 , 456.5788  , 578.054   , 486.04138 ],\n","       [428.49228 , 360.70508 , 449.75656 , 422.6173  ],\n","       [335.29684 , 214.26468 , 458.31155 , 437.45132 ],\n","       [390.02158 , 213.99994 , 458.17075 , 290.69315 ],\n","       [563.00653 , 461.26596 , 572.1845  , 475.11636 ],\n","       [447.49625 , 492.52808 , 468.6929  , 515.8289  ],\n","       [357.11096 , 419.66318 , 366.7544  , 442.35812 ],\n","       [330.43155 , 370.5042  , 359.13052 , 401.0913  ],\n","       [392.85696 , 230.91658 , 438.9949  , 378.30334 ],\n","       [326.4493  , 361.99042 , 371.5947  , 408.79626 ],\n","       [350.38632 , 412.82416 , 371.00473 , 446.6599  ],\n","       [330.70145 , 350.6859  , 381.10666 , 429.1433  ],\n","       [ 24.83678 , 379.83826 ,  58.810345, 419.58154 ],\n","       [702.0591  , 426.56094 , 712.6904  , 439.17044 ],\n","       [576.0404  , 172.87315 , 583.6854  , 185.58937 ],\n","       [453.941   , 499.93967 , 462.83188 , 509.85703 ],\n","       [341.26318 , 373.6578  , 371.96637 , 445.92233 ],\n","       [557.1179  , 496.74274 , 567.4221  , 506.53552 ],\n","       [353.00873 , 340.59262 , 407.4599  , 443.7413  ],\n","       [545.04144 , 445.30206 , 584.75714 , 494.226   ],\n","       [397.12497 , 234.2075  , 439.689   , 278.10556 ],\n","       [394.8986  , 309.8938  , 450.69125 , 429.0517  ],\n","       [350.60916 , 332.68137 , 433.842   , 399.05014 ],\n","       [359.8998  , 390.25476 , 370.07382 , 412.14948 ],\n","       [385.33145 , 182.60492 , 449.90744 , 358.14624 ],\n","       [360.31522 , 352.45557 , 451.2477  , 426.23688 ]], dtype=float32), 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': array([0.8237559 , 0.77592325, 0.7583789 , 0.68835837, 0.68630356,\n","       0.6819499 , 0.66018134, 0.6282103 , 0.5526963 , 0.54247385,\n","       0.46964228, 0.46603367, 0.4005548 , 0.36482188, 0.34991595,\n","       0.3394455 , 0.32426322, 0.20543928, 0.19085109, 0.17104174,\n","       0.13955945, 0.13615856, 0.12208051, 0.11873759, 0.1143681 ,\n","       0.11341105, 0.09812793, 0.08640044, 0.0834131 , 0.07848291,\n","       0.07116272, 0.07076656, 0.05879877, 0.05834527, 0.05716596],\n","      dtype=float32)}, {'boxes': array([[335.0481   , 419.22827  , 357.2599   , 489.18604  ],\n","       [540.2884   ,  84.72339  , 691.2729   , 192.73721  ],\n","       [442.6536   , 526.161    , 490.1807   , 600.0404   ],\n","       [328.5861   , 405.70377  , 366.2573   , 500.89096  ],\n","       [568.9524   , 117.55779  , 657.2013   , 193.30383  ],\n","       [529.2874   ,  85.436874 , 694.6003   , 297.0766   ],\n","       [  5.6663904, 435.24295  ,  16.054237 , 452.44638  ],\n","       [183.89522  ,  33.1731   , 192.41643  ,  48.334866 ],\n","       [444.29517  , 163.4711   , 591.5342   , 599.9596   ],\n","       [  2.5636053, 432.32068  ,  19.057053 , 455.3421   ]],\n","      dtype=float32), 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'scores': array([0.9507573 , 0.82821184, 0.1730766 , 0.15676378, 0.10699852,\n","       0.0843601 , 0.08325122, 0.0578876 , 0.05411881, 0.05307782],\n","      dtype=float32)}]\n"]}],"source":["for prediction in predictions:\n","    prediction['boxes'] = prediction['boxes'].cpu().numpy()\n","    prediction['labels'] = prediction['labels'].cpu().numpy()\n","    prediction['scores'] = prediction['scores'].cpu().numpy()\n","\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1720109167456,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"bNiBohgsHtX_","outputId":"c0599c65-3e61-4f47-a52f-d0f66fc909ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Example Calculation...\n","Average Precision: 0.50, Average Recall: 1.00, Average F1 Score: 0.67, Average mAP: 0.28\n"]}],"source":["import numpy as np\n","\n","# Example data for multiple images\n","print(\"Example Calculation...\")\n","images_predictions = [\n","    [\n","        {'label': 0, 'probability': 0.9, 'bounding_box': [100, 100, 50, 50]},  # label 0, high confidence\n","        {'label': 1, 'probability': 0.8, 'bounding_box': [200, 200, 50, 50]},  # label 1, high confidence\n","        {'label': 0, 'probability': 0.7, 'bounding_box': [120, 120, 40, 40]},  # label 0, medium confidence\n","        {'label': 1, 'probability': 0.6, 'bounding_box': [210, 210, 60, 60]},  # label 1, medium confidence\n","    ],\n","    [\n","        {'label': 0, 'probability': 0.85, 'bounding_box': [95, 95, 55, 55]},  # label 0, high confidence\n","        {'label': 1, 'probability': 0.75, 'bounding_box': [210, 210, 45, 45]},  # label 1, high confidence\n","        {'label': 0, 'probability': 0.65, 'bounding_box': [125, 125, 35, 35]},  # label 0, medium confidence\n","        {'label': 1, 'probability': 0.55, 'bounding_box': [200, 200, 55, 55]},  # label 1, medium confidence\n","    ]\n","]\n","\n","images_ground_truths = [\n","    [\n","        {'label': 0, 'bounding_box': [105, 105, 60, 60]},  # label 0\n","        {'label': 1, 'bounding_box': [200, 200, 50, 50]},  # label 1\n","    ],\n","    [\n","        {'label': 0, 'bounding_box': [100, 100, 60, 60]},  # label 0\n","        {'label': 1, 'bounding_box': [210, 210, 50, 50]},  # label 1\n","    ]\n","]\n","\n","def evaluate_object_detection_multiple_images(images_predictions, images_ground_truths, prob_threshold=0.5, overlap_threshold=0.5):\n","    \"\"\"\n","    Evaluate object detection predictions for multiple images.\n","\n","    Args:\n","    - images_predictions (list of lists): List where each element is a list of dictionaries containing prediction data for one image.\n","      Each dictionary should have keys 'label', 'probability', 'bounding_box'.\n","      Example format for one image:\n","      [{'label': 'car', 'probability': 0.92, 'bounding_box': [x, y, width, height]}, ...]\n","\n","    - images_ground_truths (list of lists): List where each element is a list of dictionaries containing ground truth data for one image.\n","      Each dictionary should have keys 'label' and 'bounding_box'.\n","      Example format for one image:\n","      [{'label': 'car', 'bounding_box': [xmin, ymin, width, height]}, ...]\n","\n","    - prob_threshold (float): Minimum probability threshold for predictions.\n","\n","    - overlap_threshold (float): Minimum IoU threshold for considering a detection as correct.\n","\n","    Returns:\n","    - precision (float): Average Precision score across all images.\n","    - recall (float): Average Recall score across all images.\n","    - f1_score (float): Average F1 score across all images.\n","    - mAP (float): mean Average Precision (mAP) score across all images.\n","    \"\"\"\n","\n","    def calculate_precision_recall(predictions, ground_truths, prob_threshold, overlap_threshold):\n","        # Filter predictions based on probability threshold\n","        predictions = [pred for pred in predictions if pred['probability'] >= prob_threshold]\n","\n","        # Initialize variables\n","        true_positives = 0\n","        false_positives = len(predictions)\n","        false_negatives = len(ground_truths)\n","\n","        for gt in ground_truths:\n","            found_match = False\n","            for pred in predictions:\n","                if pred['label'] == gt['label']:\n","                    iou = calculate_iou(pred['bounding_box'], gt['bounding_box'])\n","                    if iou >= overlap_threshold:\n","                        found_match = True\n","                        break\n","\n","            if found_match:\n","                true_positives += 1\n","                false_positives -= 1\n","                false_negatives -= 1\n","\n","        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n","        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n","\n","        return precision, recall\n","\n","    def calculate_iou(boxA, boxB):\n","        # Convert to (x1, y1, x2, y2) format\n","        x1A, y1A, wA, hA = boxA[0], boxA[1], boxA[2], boxA[3]\n","        x1B, y1B, wB, hB = boxB[0], boxB[1], boxB[2], boxB[3]\n","        x2A, y2A = x1A + wA, y1A + hA\n","        x2B, y2B = x1B + wB, y1B + hB\n","\n","        # Calculate intersection area\n","        xA = max(x1A, x1B)\n","        yA = max(y1A, y1B)\n","        xB = min(x2A, x2B)\n","        yB = min(y2A, y2B)\n","\n","        inter_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\n","        # Calculate area of each box\n","        boxAArea = wA * hA\n","        boxBArea = wB * hB\n","\n","        # Calculate union area\n","        union_area = boxAArea + boxBArea - inter_area\n","\n","        # Calculate IoU\n","        iou = inter_area / union_area if union_area > 0 else 0\n","\n","        return iou\n","\n","    def calculate_f1_score(precision, recall):\n","        if precision + recall == 0:\n","            return 0\n","        f1_score = 2 * (precision * recall) / (precision + recall)\n","        return f1_score\n","\n","    def calculate_map(predictions, ground_truths, prob_threshold, overlap_threshold):\n","        average_precision = []\n","        num_classes = len(set([gt['label'] for gt in ground_truths]))\n","\n","        for c in range(num_classes):\n","            class_predictions = [pred for pred in predictions if pred['label'] == c]\n","            class_ground_truths = [gt for gt in ground_truths if gt['label'] == c]\n","\n","            precisions = []\n","            recalls = []\n","\n","            for threshold in np.arange(0.5, 1.0, 0.05):  # Vary IoU threshold from 0.5 to 0.95\n","                precisions_at_threshold = []\n","                recalls_at_threshold = []\n","\n","                for prob_thresh in np.arange(0.0, 1.05, 0.05):  # Vary confidence threshold from 0 to 1\n","                    precision, recall = calculate_precision_recall(class_predictions, class_ground_truths,\n","                                                                   prob_threshold=prob_thresh,\n","                                                                   overlap_threshold=threshold)\n","                    precisions_at_threshold.append(precision)\n","                    recalls_at_threshold.append(recall)\n","\n","                avg_precision = np.mean(precisions_at_threshold)\n","                precisions.append(avg_precision)\n","                recalls.append(np.mean(recalls_at_threshold))\n","\n","            average_precision.append(np.mean(precisions))\n","\n","        mAP = np.mean(average_precision)\n","\n","        return mAP\n","\n","    total_precision = 0\n","    total_recall = 0\n","    total_f1_score = 0\n","    total_mAP = 0\n","\n","    num_images = len(images_predictions)\n","\n","    for i in range(num_images):\n","        predictions = images_predictions[i]\n","        ground_truths = images_ground_truths[i]\n","\n","        # Calculate Precision and Recall for the current image\n","        precision, recall = calculate_precision_recall(predictions, ground_truths, prob_threshold, overlap_threshold)\n","\n","        # Calculate F1 score for the current image\n","        f1_score = calculate_f1_score(precision, recall)\n","\n","        # Calculate mAP for the current image\n","        mAP = calculate_map(predictions, ground_truths, prob_threshold, overlap_threshold)\n","\n","        # Accumulate metrics for averaging\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1_score += f1_score\n","        total_mAP += mAP\n","\n","    # Average metrics across all images\n","    precision_avg = total_precision / num_images\n","    recall_avg = total_recall / num_images\n","    f1_score_avg = total_f1_score / num_images\n","    mAP_avg = total_mAP / num_images\n","\n","    return precision_avg, recall_avg, f1_score_avg, mAP_avg\n","\n","\n","# Evaluate object detection for multiple images\n","precision_avg, recall_avg, f1_score_avg, mAP_avg = evaluate_object_detection_multiple_images(images_predictions, images_ground_truths, prob_threshold=0.5, overlap_threshold=0.3)\n","\n","print(f'Average Precision: {precision_avg:.2f}, Average Recall: {recall_avg:.2f}, Average F1 Score: {f1_score_avg:.2f}, Average mAP: {mAP_avg:.2f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1720109167456,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"Zjmkt_Z7a1xf","outputId":"ccf069ae-92f5-47c2-fcd6-3ab6b339dd58"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[{'label': 1, 'bounding_box': [367.3354797363281, 340.1585388183594, 33.22998046875, 34.634063720703125]}, {'label': 1, 'bounding_box': [333.2627258300781, 373.3300476074219, 20.18621826171875, 18.196044921875]}, {'label': 1, 'bounding_box': [409.2523498535156, 365.2770080566406, 38.29656982421875, 63.66180419921875]}, {'label': 1, 'bounding_box': [395.2663269042969, 232.90467834472656, 65.37789916992188, 57.08122253417969]}], [{'label': 1, 'bounding_box': [332.43280029296875, 417.6826477050781, 26.33416748046875, 73.948486328125]}]]\n"]}],"source":["def convert_ground_truths(ground_truths):\n","    images_ground_truths = []\n","\n","    for ground_truth in ground_truths:\n","        image_gt = []\n","        boxes = ground_truth['boxes']\n","        labels = ground_truth['labels']\n","\n","        for i in range(len(labels)):\n","            label = int(labels[i])  # Convert label to integer\n","            box = boxes[i].tolist()  # Convert numpy array to list\n","            xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n","            width = xmax - xmin\n","            height = ymax - ymin\n","            bounding_box = [xmin, ymin, width, height]\n","\n","            image_gt.append({'label': label, 'bounding_box': bounding_box})\n","\n","        images_ground_truths.append(image_gt)\n","\n","    return images_ground_truths\n","\n","converted_ground_truths = convert_ground_truths(ground_truths)\n","print(converted_ground_truths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1720109167457,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"v6s1E6e8cMZm","outputId":"a3b12eaa-4056-48ef-8b91-cf27f2dbd8af"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[{'label': 1, 'probability': 0.8237559199333191, 'bounding_box': [409, 357, 39, 73]}, {'label': 1, 'probability': 0.7759232521057129, 'bounding_box': [392, 210, 68, 209]}, {'label': 1, 'probability': 0.7583789229393005, 'bounding_box': [364, 338, 37, 43]}, {'label': 1, 'probability': 0.6883583664894104, 'bounding_box': [561, 459, 12, 20]}, {'label': 1, 'probability': 0.6863035559654236, 'bounding_box': [328, 330, 116, 107]}, {'label': 1, 'probability': 0.6819499135017395, 'bounding_box': [331, 374, 18, 17]}, {'label': 1, 'probability': 0.6601813435554504, 'bounding_box': [451, 496, 13, 15]}, {'label': 1, 'probability': 0.6282103061676025, 'bounding_box': [327, 337, 79, 70]}, {'label': 1, 'probability': 0.5526962876319885, 'bounding_box': [33, 388, 19, 25]}, {'label': 1, 'probability': 0.5424738526344299, 'bounding_box': [557, 456, 20, 29]}, {'label': 1, 'probability': 0.4696422815322876, 'bounding_box': [428, 360, 21, 61]}, {'label': 1, 'probability': 0.4660336673259735, 'bounding_box': [335, 214, 123, 223]}, {'label': 1, 'probability': 0.4005548059940338, 'bounding_box': [390, 213, 68, 76]}, {'label': 1, 'probability': 0.3648218810558319, 'bounding_box': [563, 461, 9, 13]}, {'label': 1, 'probability': 0.3499159514904022, 'bounding_box': [447, 492, 21, 23]}, {'label': 1, 'probability': 0.3394455015659332, 'bounding_box': [357, 419, 9, 22]}, {'label': 1, 'probability': 0.32426321506500244, 'bounding_box': [330, 370, 28, 30]}, {'label': 1, 'probability': 0.20543928444385529, 'bounding_box': [392, 230, 46, 147]}, {'label': 1, 'probability': 0.190851092338562, 'bounding_box': [326, 361, 45, 46]}, {'label': 1, 'probability': 0.17104174196720123, 'bounding_box': [350, 412, 20, 33]}, {'label': 1, 'probability': 0.13955944776535034, 'bounding_box': [330, 350, 50, 78]}, {'label': 1, 'probability': 0.1361585557460785, 'bounding_box': [24, 379, 33, 39]}, {'label': 1, 'probability': 0.12208051234483719, 'bounding_box': [702, 426, 10, 12]}, {'label': 1, 'probability': 0.11873759329319, 'bounding_box': [576, 172, 7, 12]}, {'label': 1, 'probability': 0.11436809599399567, 'bounding_box': [453, 499, 8, 9]}, {'label': 1, 'probability': 0.1134110540151596, 'bounding_box': [341, 373, 30, 72]}, {'label': 1, 'probability': 0.09812793135643005, 'bounding_box': [557, 496, 10, 9]}, {'label': 1, 'probability': 0.08640044182538986, 'bounding_box': [353, 340, 54, 103]}, {'label': 1, 'probability': 0.08341310173273087, 'bounding_box': [545, 445, 39, 48]}, {'label': 1, 'probability': 0.07848291099071503, 'bounding_box': [397, 234, 42, 43]}, {'label': 1, 'probability': 0.07116271555423737, 'bounding_box': [394, 309, 55, 119]}, {'label': 1, 'probability': 0.07076656073331833, 'bounding_box': [350, 332, 83, 66]}, {'label': 1, 'probability': 0.05879877135157585, 'bounding_box': [359, 390, 10, 21]}, {'label': 1, 'probability': 0.05834526568651199, 'bounding_box': [385, 182, 64, 175]}, {'label': 1, 'probability': 0.0571659617125988, 'bounding_box': [360, 352, 90, 73]}], [{'label': 1, 'probability': 0.9507573246955872, 'bounding_box': [335, 419, 22, 69]}, {'label': 1, 'probability': 0.8282118439674377, 'bounding_box': [540, 84, 150, 108]}, {'label': 1, 'probability': 0.1730765998363495, 'bounding_box': [442, 526, 47, 73]}, {'label': 1, 'probability': 0.15676377713680267, 'bounding_box': [328, 405, 37, 95]}, {'label': 1, 'probability': 0.1069985181093216, 'bounding_box': [568, 117, 88, 75]}, {'label': 1, 'probability': 0.08436010032892227, 'bounding_box': [529, 85, 165, 211]}, {'label': 1, 'probability': 0.0832512229681015, 'bounding_box': [5, 435, 10, 17]}, {'label': 1, 'probability': 0.05788760259747505, 'bounding_box': [183, 33, 8, 15]}, {'label': 1, 'probability': 0.0541188083589077, 'bounding_box': [444, 163, 147, 436]}, {'label': 1, 'probability': 0.0530778169631958, 'bounding_box': [2, 432, 16, 23]}]]\n"]}],"source":["def convert_predictions(predictions):\n","    images_predictions = []\n","\n","    for image_pred in predictions:\n","        image_predictions = []\n","        boxes = image_pred['boxes']\n","        labels = image_pred['labels']\n","        scores = image_pred['scores']\n","\n","        for i in range(len(labels)):\n","            label = int(labels[i])  # Convert label to integer\n","            score = float(scores[i])  # Convert score to float\n","            box = boxes[i].tolist()  # Convert numpy array to list\n","            xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n","            width = xmax - xmin\n","            height = ymax - ymin\n","            bounding_box = [int(xmin), int(ymin), int(width), int(height)]  # Convert to integers\n","\n","            image_predictions.append({\n","                'label': label,\n","                'probability': score,\n","                'bounding_box': bounding_box\n","            })\n","\n","        images_predictions.append(image_predictions)\n","\n","    return images_predictions\n","\n","converted_predictions = convert_predictions(predictions)\n","print(converted_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1720109315098,"user":{"displayName":"Sanhyew Ng","userId":"11208097760017685523"},"user_tz":180},"id":"iW6nAPRseJ_n","outputId":"086ba16b-3035-41c3-d412-c6750c312492"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performance Metrics @ probability_threshold=0.5, overlap_threshold=0.3\n","\t Precision: 0.40, \n","\t Recall: 0.88, \n","\t F1 Score: 0.55, \n","\t mAP: 0.00\n"]}],"source":["probability_threshold=0.5\n","overlap_threshold=0.3\n","\n","precision_avg, recall_avg, f1_score_avg, mAP_avg = evaluate_object_detection_multiple_images(\n","    converted_predictions,\n","    converted_ground_truths,\n","    prob_threshold=probability_threshold,\n","    overlap_threshold=overlap_threshold\n","    )\n","print(f'Performance Metrics @ probability_threshold={probability_threshold}, overlap_threshold={overlap_threshold}')\n","print(f'\\t Precision: {precision_avg:.2f}, \\n\\t Recall: {recall_avg:.2f}, \\n\\t F1 Score: {f1_score_avg:.2f}, \\n\\t mAP: {mAP_avg:.2f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_esuAUD0KCS5","outputId":"db418d3d-1435-4b57-b814-b19525aee57d"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 158/158 [38:37<00:00, 14.67s/it]"]},{"name":"stdout","output_type":"stream","text":["Performance Metrics @ probability_threshold=0.5, overlap_threshold=0.3\n","\t Precision: 0.40, \n","\t Recall: 0.88, \n","\t F1 Score: 0.55, \n","\t mAP: 0.00\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    ground_truths_all = []\n","    predictions_all = []\n","\n","    with torch.no_grad():\n","        for images, targets in tqdm(dataloader):\n","            images = list(image.to(device) for image in images)\n","\n","            ground_truths = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","            ground_truths_all.extend(ground_truths)\n","\n","            predictions = model(images)\n","            predictions_all.extend(predictions)\n","\n","    # Convert ground truths and predictions to lists of dictionaries\n","    converted_ground_truths_all = convert_ground_truths(ground_truths_all)\n","    converted_predictions_all = convert_predictions(predictions_all)\n","\n","    precision_avg, recall_avg, f1_score_avg, mAP_avg = evaluate_object_detection_multiple_images(\n","        converted_predictions,\n","        converted_ground_truths,\n","        prob_threshold=probability_threshold,\n","        overlap_threshold=overlap_threshold\n","        )\n","    print()\n","    print(f'Performance Metrics @ probability_threshold={probability_threshold}, overlap_threshold={overlap_threshold}')\n","    print(f'\\t Precision: {precision_avg:.2f}, \\n\\t Recall: {recall_avg:.2f}, \\n\\t F1 Score: {f1_score_avg:.2f}, \\n\\t mAP: {mAP_avg:.2f}')\n","\n","\n","# Example usage of evaluation function\n","evaluate_model(model, test_dataloader, device)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}